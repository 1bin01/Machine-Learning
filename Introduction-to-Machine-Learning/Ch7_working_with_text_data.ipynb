{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMasWfSBuDMOlXfCu8mNYvG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1bin01/Machine-Learning/blob/main/Introduction-to-Machine-Learning/Ch7_working_with_text_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA8kgLDIc6QX",
        "outputId": "2051dc0b-6a44-4770-b500-6d57b0499491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting mglearn\n",
            "  Downloading mglearn-0.1.9.tar.gz (540 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.1/540.1 KB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from mglearn) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from mglearn) (3.5.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from mglearn) (1.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from mglearn) (1.3.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from mglearn) (8.4.0)\n",
            "Requirement already satisfied: cycler in /usr/local/lib/python3.8/dist-packages (from mglearn) (0.11.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from mglearn) (2.9.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from mglearn) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mglearn) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mglearn) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mglearn) (3.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mglearn) (23.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib->mglearn) (4.38.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->mglearn) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->mglearn) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->mglearn) (1.10.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib->mglearn) (1.15.0)\n",
            "Building wheels for collected packages: mglearn\n",
            "  Building wheel for mglearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mglearn: filename=mglearn-0.1.9-py2.py3-none-any.whl size=582637 sha256=75ff15fe828da2f0469e2cad8050e4d582bdc24d41ef183b331c640728be593b\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/75/37/404e66d0c4bad150f101c9a0914b11a8eccc2681559936e7f7\n",
            "Successfully built mglearn\n",
            "Installing collected packages: mglearn\n",
            "Successfully installed mglearn-0.1.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting joblib==1.1.0\n",
            "  Downloading joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.0/307.0 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: joblib\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.2.0\n",
            "    Uninstalling joblib-1.2.0:\n",
            "      Successfully uninstalled joblib-1.2.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "scikit-learn 1.2.1 requires joblib>=1.1.1, but you have joblib 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed joblib-1.1.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m76.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  fonts-nanum\n",
            "0 upgraded, 1 newly installed, 0 to remove and 22 not upgraded.\n",
            "Need to get 9,599 kB of archives.\n",
            "After this operation, 29.6 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/universe amd64 fonts-nanum all 20180306-3 [9,599 kB]\n",
            "Fetched 9,599 kB in 1s (6,831 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package fonts-nanum.\n",
            "(Reading database ... 128215 files and directories currently installed.)\n",
            "Preparing to unpack .../fonts-nanum_20180306-3_all.deb ...\n",
            "Unpacking fonts-nanum (20180306-3) ...\n",
            "Setting up fonts-nanum (20180306-3) ...\n",
            "Processing triggers for fontconfig (2.13.1-2ubuntu3) ...\n",
            "/usr/share/fonts: caching, new cache contents: 0 fonts, 1 dirs\n",
            "/usr/share/fonts/truetype: caching, new cache contents: 0 fonts, 3 dirs\n",
            "/usr/share/fonts/truetype/humor-sans: caching, new cache contents: 1 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/liberation: caching, new cache contents: 16 fonts, 0 dirs\n",
            "/usr/share/fonts/truetype/nanum: caching, new cache contents: 10 fonts, 0 dirs\n",
            "/usr/local/share/fonts: caching, new cache contents: 0 fonts, 0 dirs\n",
            "/root/.local/share/fonts: skipping, no such directory\n",
            "/root/.fonts: skipping, no such directory\n",
            "/usr/share/fonts/truetype: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/humor-sans: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/liberation: skipping, looped directory detected\n",
            "/usr/share/fonts/truetype/nanum: skipping, looped directory detected\n",
            "/var/cache/fontconfig: cleaning cache directory\n",
            "/root/.cache/fontconfig: not cleaning non-existent cache directory\n",
            "/root/.fontconfig: not cleaning non-existent cache directory\n",
            "fc-cache: succeeded\n"
          ]
        }
      ],
      "source": [
        "!pip install mglearn\n",
        "!pip install --upgrade joblib==1.1.0\n",
        "!pip install -q --upgrade scikit-learn==1.0.2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import mglearn\n",
        "\n",
        "from importlib import reload  # plt 다시 그릴 때\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning) # FutureWarning 경고 제거 (없어도 문제는 x)\n",
        "\n",
        "# 나눔 포트 설정 (런타임 다시 시작 해줘야됨!)\n",
        "import matplotlib as mpl\n",
        "!sudo apt-get install -y fonts-nanum\n",
        "!sudo fc-cache -fv\n",
        "!rm ~/.cache/matplotlib -rf\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "mpl.rc('axes', unicode_minus=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.2 예제 애플리케이션: 영화 리뷰 감성 분석"
      ],
      "metadata": {
        "id": "htlMM6Gpdh-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "\n",
        "if not os.path.isfile('data/aclImdb_v1.tar.gz'):\n",
        "    !wget -q http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz -P data\n",
        "    !tar -xzf data/aclImdb_v1.tar.gz -C data\n",
        "!rm -r data/aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "p1GsDazKdlGX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_files\n",
        "\n",
        "reviews_train = load_files(\"data/aclImdb/train/\")\n",
        "text_train, y_train = reviews_train.data, reviews_train.target\n",
        "print(\"text_train의 타입:\", type(text_train))\n",
        "print(\"text_train의 길이:\", len(text_train))\n",
        "print(\"text_train[6]:\\n\", text_train[6])"
      ],
      "metadata": {
        "id": "s2rXeJYabeHo",
        "outputId": "c4a2d362-ba3e-44dc-8c41-8d06596f0415",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_train의 타입: <class 'list'>\n",
            "text_train의 길이: 25000\n",
            "text_train[6]:\n",
            " b\"This movie has a special way of telling the story, at first i found it rather odd as it jumped through time and I had no idea whats happening.<br /><br />Anyway the story line was although simple, but still very real and touching. You met someone the first time, you fell in love completely, but broke up at last and promoted a deadly agony. Who hasn't go through this? but we will never forget this kind of pain in our life. <br /><br />I would say i am rather touched as two actor has shown great performance in showing the love between the characters. I just wish that the story could be a happy ending.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# <br> (개행) 삭제하기\n",
        "text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]"
      ],
      "metadata": {
        "id": "OkKjJf4WbyLu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"class 별 샘플 수 : \", np.bincount(y_train))"
      ],
      "metadata": {
        "id": "TTb_Td4OcHPg",
        "outputId": "75282849-7a60-4e71-c1a9-243f6da4c377",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "class 별 샘플 수 :  [12500 12500]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews_test = load_files(\"data/aclImdb/test/\")\n",
        "text_test, y_test = reviews_test.data, reviews_test.target\n",
        "\n",
        "print(\"text_text의 길이:\", len(text_train))\n",
        "print(\"class 별 샘플 수 : \", np.bincount(y_train))\n",
        "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
      ],
      "metadata": {
        "id": "Wwc9GG77cMJb",
        "outputId": "f81a1010-3a23-4dc3-9583-5aa0550e924f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text_text의 길이: 25000\n",
            "class 별 샘플 수 :  [12500 12500]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.3 텍스트 데이터를 BOW로 표현하기"
      ],
      "metadata": {
        "id": "HldjW2szcoAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 단어가 얼마나 자주 등장하는 지만 헤아림\n",
        "\n",
        "# 7.3.1 샘플 데이터에 BOW 적용하기\n",
        "bards_words =[\"The fool doth think he is wise,\",\n",
        "              \"but the wise man knows himself to be a fool\"]\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vect = CountVectorizer()\n",
        "vect.fit(bards_words)\n",
        "\n",
        "print(\"어희 사전의 크기 : \", len(vect.vocabulary_))\n",
        "print(\"어휘 사전의 내용 : \", vect.vocabulary_)"
      ],
      "metadata": {
        "id": "8E1NnGbNcqwB",
        "outputId": "9a75d243-1bda-43a5-e42c-78f729f3731d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "어희 사전의 크기 :  13\n",
            "어휘 사전의 내용 :  {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words = vect.transform(bards_words)\n",
        "print(\"BOW : \", repr(bag_of_words))"
      ],
      "metadata": {
        "id": "cgEHJFphdawT",
        "outputId": "9bd9ef7e-e216-4a60-c6fd-9b0bea11bba2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOW :  <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
            "\twith 16 stored elements in Compressed Sparse Row format>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# toarray 를 사용해 희소배열에서 Nunpy 배열로 바꿈\n",
        "print(\"BOW의 밀집 표현 : \\n\", bag_of_words.toarray())"
      ],
      "metadata": {
        "id": "sOdwRzrqdtAb",
        "outputId": "d2ecabee-078c-4534-c768-4cbf0c9e39a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOW의 밀집 표현 : \n",
            " [[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
            " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.3.2 영화 리뷰에 대한 BOW\n",
        "\n",
        "vect = CountVectorizer().fit(text_train)\n",
        "X_train = vect.transform(text_train)\n",
        "print(\"X_train :\\n\", repr(X_train))"
      ],
      "metadata": {
        "id": "godrHKLhd6Oi",
        "outputId": "9177bb81-94d1-42e1-ffd3-51b70e0a1513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train :\n",
            " <25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
            "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vect.get_feature_names_out()\n",
        "print(\"특성의 개수 : \", len(feature_names))\n",
        "print(\"처음 20개 특성 :\\n\", feature_names[:20])"
      ],
      "metadata": {
        "id": "cKz4w68zeZHW",
        "outputId": "e15d6008-d10c-4707-df91-466dbe26d995",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "특성의 개수 :  74849\n",
            "처음 20개 특성 :\n",
            " ['00' '000' '0000000000001' '00001' '00015' '000s' '001' '003830' '006'\n",
            " '007' '0079' '0080' '0083' '0093638' '00am' '00pm' '00s' '01' '01pm' '02']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 로지스틱 회귀를 이용해 학습시키기\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "scores = cross_val_score(LogisticRegression(max_iter = 1000), X_train, y_train, n_jobs = -1)\n",
        "print(\"교차 검증의 점수 : {:.2f}\" .format(np.mean(scores)))"
      ],
      "metadata": {
        "id": "0gpGrNvOeuSa",
        "outputId": "97f59176-aa9c-454d-f579-41fea55e54e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "교차 검증의 점수 : 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'C' : [0.001, 0.01, 0.1, 1, 10]}\n",
        "grid = GridSearchCV(LogisticRegression(max_iter = 5000), param_grid, n_jobs = -1)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"최상의 교차검증 점수 : {:.2f} \" .format(grid.best_score_))"
      ],
      "metadata": {
        "id": "FX49xP_Pfsuj",
        "outputId": "4036ae9c-c82b-40db-98cc-a960787dea35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최상의 교차검증 점수 : 0.89 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = vect.transform(text_test)\n",
        "print(\"test score : {:.2f}\" .format(grid.score(X_test, y_test)))"
      ],
      "metadata": {
        "id": "fMyAr-iJgcjP",
        "outputId": "1471ea05-b84c-4509-8536-38cfc835f323",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test score : 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# min_df : 토큰이 나타날 최소 문서 개수 지정\n",
        "vect = CountVectorizer(min_df = 5).fit(text_train)\n",
        "X_train = vect.transform(text_train)\n",
        "print(\"min df로 토큰 개수 제한 : \", repr(X_train))"
      ],
      "metadata": {
        "id": "CKaJ9BpTgkxm",
        "outputId": "ff160ee9-334d-4bf6-966a-6e3c2b463868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min df로 토큰 개수 제한 :  <25000x27271 sparse matrix of type '<class 'numpy.int64'>'\n",
            "\twith 3354014 stored elements in Compressed Sparse Row format>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = vect.get_feature_names_out()\n",
        "print(\"처음 50개 특성 : \\n\", feature_names[:50])"
      ],
      "metadata": {
        "id": "Bdzk_wsqg1zW",
        "outputId": "6858fb8b-732a-4339-c92c-7fafa2432aa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "처음 50개 특성 : \n",
            " ['00' '000' '007' '00s' '01' '02' '03' '04' '05' '06' '07' '08' '09' '10'\n",
            " '100' '1000' '100th' '101' '102' '103' '104' '105' '107' '108' '10s'\n",
            " '10th' '11' '110' '112' '116' '117' '11th' '12' '120' '12th' '13' '135'\n",
            " '13th' '14' '140' '14th' '15' '150' '15th' '16' '160' '1600' '16mm' '16s'\n",
            " '16th']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid = GridSearchCV(LogisticRegression(max_iter = 5000), param_grid, n_jobs = -1)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"최상의 교차 검증 점수 : {:.2f}\" .format(grid.best_score_))"
      ],
      "metadata": {
        "id": "vLwpJVmJhUp9",
        "outputId": "2c1849af-f6c6-41a0-9b48-2e0c5cd4ccea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최상의 교차 검증 점수 : 0.89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7.4 불용어\n",
        "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
        "print(\"불용어 개수 : \", len(ENGLISH_STOP_WORDS))"
      ],
      "metadata": {
        "id": "lL183Ur5hm2w",
        "outputId": "475f9864-8c07-49a5-9aaa-7e1dd6446831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어 개수 :  318\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect = CountVectorizer(min_df = 5, stop_words = \"english\").fit(text_train)\n",
        "X_train = vect.transform(text_train)\n",
        "print(\"불용어가 제거된 train data : \", repr(X_train))"
      ],
      "metadata": {
        "id": "Jh6Ml36JiFXm",
        "outputId": "ec6d6c0a-2832-42e9-9e59-37c68602af60",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "불용어가 제거된 train data :  <25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n",
            "\twith 2149958 stored elements in Compressed Sparse Row format>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "grid = GridSearchCV(LogisticRegression(max_iter = 5000), param_grid, n_jobs = -1)\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"최상의 교차 검증 점수 : {:.2f}\" .format(grid.best_score_))"
      ],
      "metadata": {
        "id": "lIQlYr9Rif0s",
        "outputId": "e9335dad-7662-48b4-dbc5-279a6b77497b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "최상의 교차 검증 점수 : 0.88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.5 tf_idf로 데이터 스케일 변경하기"
      ],
      "metadata": {
        "id": "fc5U50M0i3Do"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 특정 문서에 자주 나타나는 단어에 가중치를 줌\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "pipe = make_pipeline(TfidfVectorizer(min_df = 5), LogisticRegression(max_iter = 5000))\n",
        "param_grid = {'logisticregression__C' : [0.001, 0.01, 0.1, 1, 10]}\n",
        "\n",
        "grid = GridSearchCV(pipe, param_grid, n_jobs = -1)\n",
        "grid.fit(text_train, y_train)\n",
        "print(\"최상의 교차 검증 점수 : {:.2f}\" .format(grid.best_score_))"
      ],
      "metadata": {
        "id": "TTxQJHtZi5UV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}